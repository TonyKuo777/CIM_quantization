# CIM_quantization
The Analog Computing In-memory (CIM) involves high density interleaved memory arrays which are beneficial to the deep neural networks with lots of parallel computations. Besides, it can also improve the Von Neumann Bottleneck of digital logic circuits and shows potential in Artificial Intelligence (AI) accelerator to achieve high energy efficiency.
In this work, we propose a two-stage training framework, which not only considers the constraints in hardware architecture but also analyzes the nonidealities which occur in CIM devices. First, we design a CIM convolution function that is general in various neural networks. By splitting and compensating the weight kernels in convolution layers, they will not be limited to the number of accumulations in CIM architectures. In addition, the training framework can not only quantize the weights and activations to the target bit widths; but also inject noise during the training process to improve the robustness of the neural network in inference. Through the results of the experiments, ResNet can improve 2.26% and 8.95% in accuracy respectively. Moreover, due to the limitation of Analog-to-digital Converter (ADC) bit widths, it is necessary to quantize the output value of Matrix-vector Multiplication (MVM). The MVM quantizer has flexible quantization intervals to the output distribution of each layer, so it can retain important information and promote accuracy after quantization. Take ResNet and VGG models as the experimental verification, they can increase by 4.48% and 5.46% of accuracy respectively. As the results show, the proposed method is effective and useful for the fabrication and design of chip systems for Computing In-memory.
